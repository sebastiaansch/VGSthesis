{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from string import digits\n",
    "import nltk.corpus.reader.cmudict\n",
    "import nltk.corpus.reader.timit\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "timit = nltk.corpus.timit.transcription_dict()\n",
    "dict1 = {}\n",
    "def make_phoneme_dictionary(wordlist):\n",
    "    #make sure to intialize the corpus outside of the function\n",
    "    for word in wordlist:\n",
    "        remove_digits = str.maketrans('', '', digits)\n",
    "        x = [words.translate(remove_digits) for words in arpabet[word][0]]\n",
    "        dict1[word] = x\n",
    "        \n",
    "def preprocess_alignment(txtfile):\n",
    "    for idx,info in enumerate(txtfile):\n",
    "        f[idx] = info.split(\" \",-1)\n",
    "        f[idx][2] = f[idx][2].replace(\"\\n\",\"\")\n",
    "    return txtfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the tested files\n",
    "path = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/Results_isolated_word_recognition/documents/textcp.csv\"\n",
    "\n",
    "textcp = pd.read_csv(path)\n",
    "\n",
    "listtest = [\"dog\",\"man\",\"boy\",\"girl\",\"woman\",\"people\"\n",
    "            ,\"dogs\",\"shirt\",\"child\",\"ball\",\"person\"\n",
    "            ,\"children\",\"men\",\"girls\",\"bike\",\"rock\",\"camera\"\n",
    "            ,\"boys\",\"hat\",\"player\",\"jacket\",\"basketball\",\"swing\"\n",
    "            ,\"car\", \"wall\", \"hair\",\"football\",\"sunglasses\",\"head\"\n",
    "            ,\"shorts\",\"dress\",\"table\"]\n",
    "\n",
    "listtest2 = [\"dog\",\"man\",\"boy\",\"girl\",\"woman\",\"people\"\n",
    "            ,\"dogs\",\"shirt\",\"child\",\"ball\",\"person\"\n",
    "            ,\"children\",\"men\",\"girls\",\"bike\",\"rock\",\"camera\"\n",
    "            ,\"boys\",\"hat\",\"player\",\"jacket\",\"basketball\",\"swing\"\n",
    "            ,\"car\", \"wall\", \"hair\",\"football\",\"sunglasses\",\"head\"\n",
    "            ,\"shorts\",\"dress\",\"table\",\"water\",\"grass\",\"bench\",\"snow\"\n",
    "            ,\"air\",\"field\",\"street\",\"mouth\",\"dirt\",\"mountain\",\"pool\"\n",
    "            ,\"ocean\",\"sand\",\"building\",\"soccer\",\"park\",\"face\"]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def print_file_list(inputwords):\n",
    "    #loop through the words\n",
    "    word_appended_finallist = []\n",
    "    finallist = []\n",
    "    finallistdict = defaultdict(list)\n",
    "    files_and_words = defaultdict(list)\n",
    "    for inputword in inputwords:\n",
    "        wavfiles = []\n",
    "        #find captions that contain these words\n",
    "        for idx, captions in enumerate(textcp.captions):\n",
    "            if inputword in tokenizer.tokenize(captions):\n",
    "                wavfiles.append(textcp.original.iloc[idx])\n",
    "        wavfilenames = []\n",
    "        #preprocess them for file copying\n",
    "        for word in wavfiles:\n",
    "            word = word.replace(\"#\",\"_\")\n",
    "            word = word.replace(\".jpg\",\"\")\n",
    "            wordwithinputword = inputword + \"_\" + word\n",
    "            wavfilenames.append(word)\n",
    "        count = 0\n",
    "        #move the files\n",
    "        for word in wavfilenames:\n",
    "            count = count + 1\n",
    "            word_appended_finallist.append(wordwithinputword)\n",
    "            \n",
    "            finallist.append(word)\n",
    "            file = word.split('_', -1)[0:2]\n",
    "            testword = \"_\".join(file)\n",
    "            wordjpg = testword + \".jpg\"\n",
    "            files_and_words[word].append(inputword)\n",
    "            finallistdict[wordjpg].append(inputword)\n",
    "            if count == 50:\n",
    "                count = 0\n",
    "                break         \n",
    "    return wavfiles,wavfilenames,word_appended_finallist,finallist,finallistdict,files_and_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfiles,wavfilenames,word_appended_finallist,finallist,finallistdict,files_and_words = print_file_list(listtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignpath = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/labels\"\n",
    "\n",
    "# for file in os.listdir(alignpath):\n",
    "#     if(file == (\"align_\" + finallist[0]+ \".txt\")):\n",
    "#         print(file)\n",
    "#         f = open(alignpath + \"/\" + file, \"r\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# starts = [2.366]\n",
    "# stops = [2.786]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpath = \"/Volumes/Schnellheit/ThesisDatasets/Flickr_datasets/flickr_audio/wavs/\"\n",
    "# starts = np.multiply(starts,1000)\n",
    "# stops = np.multiply(stops,1000)\n",
    "# wav = \"1084040636_97d9633581_0\"+\".wav\"\n",
    "\n",
    "\n",
    "# for t1,t2 in zip(starts,stops):\n",
    "#     newAudio = AudioSegment.from_wav(testpath+wav)\n",
    "#     newAudio = newAudio[t1:t2]\n",
    "#     newAudio.export(f'{t1}newSong.wav', format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to import a phonemedictionary\n",
    "import json\n",
    "\n",
    "with open('/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/phonemedictionary.json') as json_file:\n",
    "    phonemedictionary = json.load(json_file)\n",
    "    \n",
    "# phonemedictionary[\"camera\"] = ['K', 'AE', 'M', 'AXR', 'AX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonemedictionary[\"dogs\"] = [\"D\",\"AO\",\"G\",\"Z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For turning phonemes into words and then cutting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignpath = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/labels\"\n",
    "# audiofiles = \"/Volumes/Schnellheit/ThesisDatasets/Flickr_datasets/flickr_audio/wavs/\"\n",
    "# checklist = []\n",
    "# for key, value in files_and_words.items():\n",
    "#     for file in os.listdir(alignpath):\n",
    "#         if(file == (\"align_\" + key + \".txt\")):\n",
    "#             checklist.append(file)\n",
    "#             f = open(alignpath + \"/\" + file, \"r\")\n",
    "#             f = f.readlines()\n",
    "#             f = preprocess_alignment(f)\n",
    "#             wordsfound = 0\n",
    "#             for word in value:\n",
    "#                 counter = 0\n",
    "#                 phonemes = phonemedictionary[word]\n",
    "#                 for idx,alignedphoneme in enumerate(f):\n",
    "#                     wordfinder = []\n",
    "#                     if(alignedphoneme[0] == phonemes[0]):\n",
    "#                         #create list of phonemes to the length of the phoneme looked for\n",
    "#                         for i in range(0,len(phonemes)):\n",
    "# #                             if (word == \"hair\"):\n",
    "# #                                 print(file)\n",
    "# #                                 print(wordfinder)\n",
    "#                             try:\n",
    "#                                 wordfinder.append(f[idx+i][0])\n",
    "#                             except IndexError:\n",
    "#                                 break\n",
    "#                         #if the order of phonemes that is looked for is found\n",
    "#                         if(wordfinder == phonemes):\n",
    "#                             wordsfound = wordsfound + 1\n",
    "#                             start = defaultdict(list)\n",
    "#                             stop = defaultdict(list)\n",
    "#                             #make list of start times of the phoneme\n",
    "#                             for i in range(0,len(phonemes)):\n",
    "#                                 start[f[idx+i][0]].append(float(f[idx+i][1])*1000)\n",
    "#                                 stop[f[idx+i][0]].append(float(f[idx+i][2])*1000)\n",
    "#                             wordstart = start[phonemes[0]][0]\n",
    "#                             wordend = stop[phonemes[-1]][-1]\n",
    "#                             #looks for second occurences of a word then make new filename\n",
    "#                             if (counter>0):\n",
    "#                                 newAudio = AudioSegment.from_wav(audiofiles+key+\".wav\")\n",
    "#                                 newAudio = newAudio[wordstart:wordend]\n",
    "#                                 newAudio.export(f'49words/{word}_{key}_{counter}.wav', format=\"wav\")\n",
    "#                                 break\n",
    "\n",
    "#                             newAudio = AudioSegment.from_wav(audiofiles+key+\".wav\")\n",
    "#                             newAudio = newAudio[wordstart:wordend]\n",
    "#                             newAudio.export(f'49words/{word}_{key}.wav', format=\"wav\")\n",
    "                \n",
    "#                             counter = counter + 1\n",
    "                            \n",
    "# #             print(f\"There were {len(value)} words, we found {wordsfound}/{len(value)} in file {file}\")\n",
    "# #             print(f\"We were looking for {value}\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "phonemedictionary[\"camera\"] = ['K', 'AE', 'M', 'R', 'AX']\n",
    "with open('/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/phonemedictionary.json', 'w') as f:\n",
    "    json.dump(phonemedictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera has two phoneme spellings in this set, namely: ['K', 'AE', 'M', 'AXR', 'AX'] and ['K', 'AE', 'M', 'R', 'AX']\n",
    "#dogs one instance where its spelled with an S instead of Z\n",
    "#bench and street maybe have a second spelling, most likely just annotations missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For cutting out phonemes and phoneme subwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignpath = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/labels\"\n",
    "# audiofiles = \"/Volumes/Schnellheit/ThesisDatasets/Flickr_datasets/flickr_audio/wavs/\"\n",
    "# checklist = []\n",
    "# for key, value in files_and_words.items():\n",
    "#     for file in os.listdir(alignpath):\n",
    "#         if(file == (\"align_\" + key + \".txt\")):\n",
    "#             checklist.append(file)\n",
    "#             f = open(alignpath + \"/\" + file, \"r\")\n",
    "#             f = f.readlines()\n",
    "#             f = preprocess_alignment(f)\n",
    "#             wordsfound = 0\n",
    "#             for word in value:\n",
    "#                 counter = 0\n",
    "#                 phonemes = phonemedictionary[word]\n",
    "#                 for idx,alignedphoneme in enumerate(f):\n",
    "#                     wordfinder = []\n",
    "#                     if(alignedphoneme[0] == phonemes[0]):\n",
    "#                         #create list of phonemes to the length of the phoneme looked for\n",
    "#                         for i in range(0,len(phonemes)):\n",
    "# #                             if (word == \"hair\"):\n",
    "# #                                 print(file)\n",
    "# #                                 print(wordfinder)\n",
    "#                             try:\n",
    "#                                 wordfinder.append(f[idx+i][0])\n",
    "#                             except IndexError:\n",
    "#                                 break\n",
    "#                         #if the order of phonemes that is looked for is found\n",
    "#                         if(wordfinder == phonemes):\n",
    "#                             wordsfound = wordsfound + 1\n",
    "#                             start = defaultdict(list)\n",
    "#                             stop = defaultdict(list)\n",
    "#                             #make list of start times of the phoneme\n",
    "#                             for i in range(0,len(phonemes)):\n",
    "#                                 start[f[idx+i][0]].append(float(f[idx+i][1])*1000)\n",
    "#                                 stop[f[idx+i][0]].append(float(f[idx+i][2])*1000)\n",
    "#                             for j in range(0,len(phonemes)):\n",
    "#                                 wordstart = start[phonemes[0]][0]\n",
    "#                                 wordend = stop[phonemes[j]][0]\n",
    "#                                 #if it isnt the first phoneme and the phonemes are the same take the second value in the dictionary\n",
    "#                                 if(j>0 and phonemes[0] == phonemes[j]):\n",
    "#                                     wordend = stop[phonemes[j]][1]\n",
    "#                                 #looks for second occurences of a word then make new filename\n",
    "#                                 if (counter>0):\n",
    "#                                     newAudio = AudioSegment.from_wav(audiofiles+key+\".wav\")\n",
    "#                                     newAudio = newAudio[wordstart:wordend]\n",
    "#                                     newAudio.export(f'phonemes49words/{len(phonemes[0:1+j])}_{\"\".join(phonemes[0:1+j])}_{word}_{key}_{counter}.wav', format=\"wav\")\n",
    "#                                     break\n",
    "#                                 newAudio = AudioSegment.from_wav(audiofiles+key+\".wav\")\n",
    "#                                 newAudio = newAudio[wordstart:wordend]\n",
    "#                                 newAudio.export(f'phonemes49words/{len(phonemes[0:1+j])}_{\"\".join(phonemes[0:1+j])}_{word}_{key}.wav', format=\"wav\")\n",
    "\n",
    "#                             counter = counter + 1\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignpath = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/labels\"\n",
    "\n",
    "# for file in os.listdir(alignpath):\n",
    "#     if(file == (\"align_\" + finallist[0]+ \".txt\")):\n",
    "#         print(file)\n",
    "#         f = open(alignpath + \"/\" + file, \"r\")\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alignpath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-fd6151d73bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcheckword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"align_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File found with word {value}: {file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alignpath' is not defined"
     ]
    }
   ],
   "source": [
    "#just a trick to find phonemes in this list easier\n",
    "\n",
    "listtest2 = [\"dog\",\"man\",\"boy\",\"girl\",\"woman\",\"people\"\n",
    "            ,\"dogs\",\"shirt\",\"child\",\"ball\",\"person\"\n",
    "            ,\"children\",\"men\",\"girls\",\"bike\",\"rock\",\"camera\"\n",
    "            ,\"boys\",\"hat\",\"player\",\"jacket\",\"basketball\",\"swing\"\n",
    "            ,\"car\", \"wall\", \"hair\",\"football\",\"sunglasses\",\"head\"\n",
    "            ,\"shorts\",\"dress\",\"table\",\"water\",\"grass\",\"bench\",\"snow\"\n",
    "            ,\"air\",\"field\",\"street\",\"mouth\",\"dirt\",\"mountain\",\"pool\"\n",
    "            ,\"ocean\",\"sand\",\"building\",\"soccer\",\"park\",\"face\"]\n",
    "\n",
    "checkword = 'street'\n",
    "found = False\n",
    "for key,values in files_and_words.items():\n",
    "    for value in values:\n",
    "        if (value == checkword):\n",
    "            for file in os.listdir(alignpath):\n",
    "                if(file == (\"align_\" + key + \".txt\")):\n",
    "                    print(f\"File found with word {value}: {file}\")\n",
    "                    f = open(alignpath + \"/\" + file, \"r\")\n",
    "                    \n",
    "                    found = True\n",
    "                    break\n",
    "        if(found==True):\n",
    "            break\n",
    "    if(found==True):\n",
    "        break\n",
    "f.readlines()         \n",
    "              \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/phonemedictionary.json', 'w') as f:\n",
    "#     json.dump(phonemedictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/phonemedictionarybackup.json', 'w') as f:\n",
    "#     json.dump(phonemedictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignpath = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/labels\"\n",
    "wordlist = []\n",
    "for file in os.listdir(alignpath):\n",
    "    f = open(alignpath + \"/\" + file, \"r\")\n",
    "    f = f.readlines()\n",
    "    x = preprocess_alignment(f)\n",
    "    for phones in x:\n",
    "        wordlist.append(phones[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " '+NOISE+',\n",
       " 'L',\n",
       " 'IH',\n",
       " 'T',\n",
       " 'XL',\n",
       " '+NOISE+',\n",
       " 'B',\n",
       " 'OY',\n",
       " '+HUMAN+',\n",
       " 'L',\n",
       " 'AY',\n",
       " 'Z',\n",
       " 'D',\n",
       " 'AW',\n",
       " 'N',\n",
       " 'AA',\n",
       " 'N',\n",
       " 'AX',\n",
       " '+LAUGH+',\n",
       " 'P',\n",
       " 'IH',\n",
       " 'K',\n",
       " 'N',\n",
       " 'IH',\n",
       " 'K',\n",
       " 'T',\n",
       " 'EY',\n",
       " 'B',\n",
       " 'XL',\n",
       " '+NOISE+',\n",
       " 'B',\n",
       " 'EH',\n",
       " 'N',\n",
       " 'CH',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " '+NOISE+',\n",
       " 'K',\n",
       " 'AH',\n",
       " 'P',\n",
       " 'XL',\n",
       " 'S',\n",
       " 'T',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'D',\n",
       " 'Z',\n",
       " 'K',\n",
       " 'L',\n",
       " 'OW',\n",
       " 'S',\n",
       " 'AE',\n",
       " 'T',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'W',\n",
       " 'AO',\n",
       " 'T',\n",
       " 'AXR',\n",
       " 'EH',\n",
       " 'S',\n",
       " '+BREATH+',\n",
       " 'EH',\n",
       " 'JH',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " 'M',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'B',\n",
       " 'L',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'IH',\n",
       " 'Z',\n",
       " 'S',\n",
       " 'IH',\n",
       " 'T',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'N',\n",
       " 'EH',\n",
       " 'K',\n",
       " 'S',\n",
       " 'T',\n",
       " 'AX',\n",
       " 'EY',\n",
       " 'M',\n",
       " 'AA',\n",
       " 'D',\n",
       " 'AXR',\n",
       " 'N',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'T',\n",
       " 'S',\n",
       " 'T',\n",
       " 'R',\n",
       " 'AH',\n",
       " 'K',\n",
       " 'CH',\n",
       " 'AXR',\n",
       " '+LAUGH+',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'F',\n",
       " 'R',\n",
       " 'AH',\n",
       " 'N',\n",
       " 'T',\n",
       " 'AX',\n",
       " 'V',\n",
       " 'AX',\n",
       " 'G',\n",
       " 'L',\n",
       " 'AE',\n",
       " 'S',\n",
       " '+LAUGH+',\n",
       " 'B',\n",
       " 'IH',\n",
       " 'L',\n",
       " 'D',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'L',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'W',\n",
       " 'AY',\n",
       " 'T',\n",
       " 'D',\n",
       " 'AO',\n",
       " 'G',\n",
       " 'HH',\n",
       " 'OW',\n",
       " 'L',\n",
       " 'D',\n",
       " 'Z',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'HH',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'D',\n",
       " 'XL',\n",
       " 'AX',\n",
       " 'V',\n",
       " 'AX',\n",
       " 'L',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'JH',\n",
       " 'R',\n",
       " 'EH',\n",
       " 'D',\n",
       " 'R',\n",
       " 'AW',\n",
       " 'N',\n",
       " 'D',\n",
       " 'T',\n",
       " 'OY',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'AX',\n",
       " 'F',\n",
       " 'IY',\n",
       " 'L',\n",
       " 'D',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " 'B',\n",
       " 'OY',\n",
       " '+NOISE+',\n",
       " 'P',\n",
       " 'L',\n",
       " 'EY',\n",
       " 'IX',\n",
       " 'NG',\n",
       " '+HUMAN+',\n",
       " 'IH',\n",
       " 'N',\n",
       " '+LAUGH+',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'S',\n",
       " 'N',\n",
       " 'OW',\n",
       " '+NOISE+',\n",
       " 'W',\n",
       " 'IH',\n",
       " 'DH',\n",
       " '+HUMAN+',\n",
       " 'T',\n",
       " 'UW',\n",
       " '+HUMAN+',\n",
       " 'B',\n",
       " 'L',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'D',\n",
       " 'AO',\n",
       " 'G',\n",
       " 'Z',\n",
       " '+HUMAN+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'T',\n",
       " 'UW',\n",
       " 'CH',\n",
       " 'IH',\n",
       " 'L',\n",
       " 'D',\n",
       " 'R',\n",
       " 'AX',\n",
       " 'N',\n",
       " 'B',\n",
       " 'IH',\n",
       " 'L',\n",
       " 'D',\n",
       " 'AX',\n",
       " 'S',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'D',\n",
       " 'K',\n",
       " 'AE',\n",
       " 'S',\n",
       " 'XL',\n",
       " 'AO',\n",
       " 'N',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'IY',\n",
       " 'CH',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'M',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'IH',\n",
       " 'Z',\n",
       " 'P',\n",
       " 'OW',\n",
       " 'Z',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'F',\n",
       " 'R',\n",
       " 'AH',\n",
       " 'N',\n",
       " 'T',\n",
       " 'AX',\n",
       " 'V',\n",
       " 'EY',\n",
       " 'S',\n",
       " 'M',\n",
       " 'AO',\n",
       " 'L',\n",
       " 'CH',\n",
       " 'AY',\n",
       " 'L',\n",
       " 'D',\n",
       " 'N',\n",
       " 'IY',\n",
       " 'R',\n",
       " 'AX',\n",
       " 'L',\n",
       " 'AY',\n",
       " 'T',\n",
       " 'P',\n",
       " 'OW',\n",
       " 'S',\n",
       " 'T',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'G',\n",
       " 'R',\n",
       " 'IY',\n",
       " 'N',\n",
       " 'V',\n",
       " 'OW',\n",
       " 'L',\n",
       " 'K',\n",
       " 'S',\n",
       " 'W',\n",
       " 'AE',\n",
       " 'G',\n",
       " 'AX',\n",
       " 'N',\n",
       " 'B',\n",
       " 'AH',\n",
       " 'G',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'AH',\n",
       " 'DH',\n",
       " 'AXR',\n",
       " 'V',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'T',\n",
       " 'IH',\n",
       " 'JH',\n",
       " 'K',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'Z',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'SIL',\n",
       " 'AE',\n",
       " 'D',\n",
       " 'M',\n",
       " 'AY',\n",
       " 'AXR',\n",
       " 'D',\n",
       " 'B',\n",
       " 'AY',\n",
       " 'P',\n",
       " 'IY',\n",
       " 'P',\n",
       " 'XL',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'AX',\n",
       " 'K',\n",
       " 'AO',\n",
       " 'R',\n",
       " 'T',\n",
       " 'Y',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'D',\n",
       " '+NOISE+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'M',\n",
       " 'EH',\n",
       " 'N',\n",
       " 'S',\n",
       " 'T',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'D',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'AA',\n",
       " 'N',\n",
       " 'AX',\n",
       " 'G',\n",
       " 'R',\n",
       " 'AE',\n",
       " 'S',\n",
       " 'F',\n",
       " 'IY',\n",
       " 'L',\n",
       " 'D',\n",
       " 'W',\n",
       " 'IH',\n",
       " 'TH',\n",
       " 'T',\n",
       " 'R',\n",
       " 'IY',\n",
       " 'Z',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'G',\n",
       " 'R',\n",
       " 'AW',\n",
       " 'N',\n",
       " 'D',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " '+HUMAN+',\n",
       " 'GARBAGE',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " '+HUMAN+',\n",
       " 'SIL',\n",
       " 'NG',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'M',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+NOISE+',\n",
       " 'SIL',\n",
       " '+HUMAN+',\n",
       " 'P',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'P',\n",
       " 'ER',\n",
       " 'S',\n",
       " 'AX',\n",
       " 'N',\n",
       " 'R',\n",
       " 'AY',\n",
       " 'D',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'AY',\n",
       " 'K',\n",
       " 'JH',\n",
       " 'AH',\n",
       " 'M',\n",
       " 'P',\n",
       " 'S',\n",
       " 'TH',\n",
       " 'R',\n",
       " 'UW',\n",
       " 'DH',\n",
       " 'IY',\n",
       " 'EY',\n",
       " 'R',\n",
       " 'HH',\n",
       " 'AY',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'AH',\n",
       " 'V',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'AH',\n",
       " 'M',\n",
       " 'P',\n",
       " 'IY',\n",
       " 'R',\n",
       " 'EY',\n",
       " 'S',\n",
       " 'T',\n",
       " 'R',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'D',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'K',\n",
       " 'M',\n",
       " 'AE',\n",
       " 'N',\n",
       " '+LAUGH+',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'AX',\n",
       " 'W',\n",
       " 'AY',\n",
       " 'T',\n",
       " '+LAUGH+',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'G',\n",
       " 'R',\n",
       " 'IY',\n",
       " 'N',\n",
       " 'F',\n",
       " 'EH',\n",
       " 'DH',\n",
       " 'AXR',\n",
       " 'D',\n",
       " 'M',\n",
       " 'AE',\n",
       " 'S',\n",
       " 'K',\n",
       " 'W',\n",
       " 'IH',\n",
       " 'TH',\n",
       " 'G',\n",
       " 'R',\n",
       " 'IY',\n",
       " 'N',\n",
       " 'JH',\n",
       " 'UW',\n",
       " 'XL',\n",
       " 'R',\n",
       " 'IY',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'P',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'T',\n",
       " 'S',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'M',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'IH',\n",
       " 'Z',\n",
       " 'AW',\n",
       " 'T',\n",
       " 'S',\n",
       " 'AY',\n",
       " 'D',\n",
       " 'S',\n",
       " 'T',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'D',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'AO',\n",
       " 'N',\n",
       " '+HUMAN+',\n",
       " 'S',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'D',\n",
       " 'SIL',\n",
       " 'W',\n",
       " 'IH',\n",
       " 'TH',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'R',\n",
       " 'AY',\n",
       " 'T',\n",
       " '+LAUGH+',\n",
       " 'B',\n",
       " 'L',\n",
       " 'UW',\n",
       " 'S',\n",
       " 'K',\n",
       " 'AY',\n",
       " 'AX',\n",
       " 'R',\n",
       " 'AW',\n",
       " 'N',\n",
       " 'D',\n",
       " 'HH',\n",
       " 'IH',\n",
       " 'M',\n",
       " '+NOISE+',\n",
       " 'SIL',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'L',\n",
       " 'EY',\n",
       " 'D',\n",
       " 'IY',\n",
       " 'IH',\n",
       " 'Z',\n",
       " 'W',\n",
       " 'EY',\n",
       " 'R',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'AX',\n",
       " '+HUMAN+',\n",
       " 'GARBAGE',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " '+HUMAN+',\n",
       " 'SIL',\n",
       " 'D',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'ER',\n",
       " 'SIL',\n",
       " 'S',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'UW',\n",
       " 'EH',\n",
       " 'T',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'ER',\n",
       " 'SIL',\n",
       " 'M',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'T',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " 'D',\n",
       " 'ER',\n",
       " 'T',\n",
       " 'IY',\n",
       " 'CH',\n",
       " 'AY',\n",
       " 'L',\n",
       " 'D',\n",
       " 'P',\n",
       " 'L',\n",
       " 'EY',\n",
       " 'Z',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'HH',\n",
       " 'AW',\n",
       " 'S',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " 'D',\n",
       " 'AO',\n",
       " 'G',\n",
       " 'IH',\n",
       " 'Z',\n",
       " 'P',\n",
       " 'L',\n",
       " 'EY',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'W',\n",
       " 'AO',\n",
       " 'T',\n",
       " 'AXR',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'JH',\n",
       " 'AA',\n",
       " 'G',\n",
       " 'IH',\n",
       " 'Z',\n",
       " 'JH',\n",
       " 'AH',\n",
       " 'M',\n",
       " 'P',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'DH',\n",
       " 'IY',\n",
       " 'EY',\n",
       " 'R',\n",
       " 'T',\n",
       " 'AX',\n",
       " 'K',\n",
       " 'AE',\n",
       " 'CH',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'AO',\n",
       " 'L',\n",
       " '+HUMAN+',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'Y',\n",
       " 'AA',\n",
       " 'R',\n",
       " 'D',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " 'B',\n",
       " 'L',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'D',\n",
       " 'AO',\n",
       " 'G',\n",
       " '+LAUGH+',\n",
       " 'IH',\n",
       " 'N',\n",
       " 'AX',\n",
       " 'R',\n",
       " 'EH',\n",
       " 'D',\n",
       " 'K',\n",
       " 'AA',\n",
       " 'L',\n",
       " 'AXR',\n",
       " 'S',\n",
       " 'P',\n",
       " 'L',\n",
       " 'AE',\n",
       " 'SH',\n",
       " 'IX',\n",
       " 'NG',\n",
       " '+NOISE+',\n",
       " 'TH',\n",
       " 'R',\n",
       " 'UW',\n",
       " 'DH',\n",
       " 'AX',\n",
       " 'W',\n",
       " 'AO',\n",
       " 'T',\n",
       " 'AXR',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'AX',\n",
       " '+LAUGH+',\n",
       " 'G',\n",
       " 'ER',\n",
       " 'L',\n",
       " 'SIL',\n",
       " 'IH',\n",
       " 'N',\n",
       " '+NOISE+',\n",
       " 'AX',\n",
       " 'F',\n",
       " 'R',\n",
       " 'IH',\n",
       " 'L',\n",
       " 'IY',\n",
       " 'SIL',\n",
       " 'P',\n",
       " 'IH',\n",
       " 'NG',\n",
       " 'K',\n",
       " 'SIL',\n",
       " 'D',\n",
       " 'R',\n",
       " 'EH',\n",
       " 'S',\n",
       " 'SIL',\n",
       " 'D',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'S',\n",
       " 'IX',\n",
       " 'NG',\n",
       " '+NOISE+',\n",
       " 'AO',\n",
       " 'N',\n",
       " 'G',\n",
       " 'R',\n",
       " 'AE',\n",
       " 'S',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'D',\n",
       " 'AO',\n",
       " 'G',\n",
       " 'Z',\n",
       " 'R',\n",
       " 'EY',\n",
       " 'S',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'AA',\n",
       " 'N',\n",
       " 'EY',\n",
       " 'T',\n",
       " 'R',\n",
       " 'AE',\n",
       " 'K',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'T',\n",
       " 'UW',\n",
       " 'P',\n",
       " 'IY',\n",
       " 'P',\n",
       " 'XL',\n",
       " '+NOISE+',\n",
       " 'W',\n",
       " 'EY',\n",
       " 'R',\n",
       " 'IX',\n",
       " 'NG',\n",
       " '+NOISE+',\n",
       " 'L',\n",
       " 'AO',\n",
       " 'NG',\n",
       " 'SIL',\n",
       " 'S',\n",
       " 'L',\n",
       " 'IY',\n",
       " 'V',\n",
       " 'Z',\n",
       " 'AE',\n",
       " 'N',\n",
       " '+NOISE+',\n",
       " 'HH',\n",
       " 'EH',\n",
       " 'L',\n",
       " 'M',\n",
       " 'AX',\n",
       " 'T',\n",
       " 'S',\n",
       " '+NOISE+',\n",
       " 'R',\n",
       " 'AY',\n",
       " 'D',\n",
       " 'B',\n",
       " 'AY',\n",
       " 'K',\n",
       " 'S',\n",
       " '+BREATH+',\n",
       " 'D',\n",
       " 'AW',\n",
       " 'N',\n",
       " 'HH',\n",
       " 'IH',\n",
       " 'L',\n",
       " '+NOISE+',\n",
       " '+BREATH+',\n",
       " 'EY',\n",
       " 'G',\n",
       " 'R',\n",
       " 'UW',\n",
       " 'P',\n",
       " 'AX',\n",
       " 'F',\n",
       " 'P',\n",
       " 'IY',\n",
       " 'P',\n",
       " 'XL',\n",
       " '+BREATH+',\n",
       " 'EH',\n",
       " 'N',\n",
       " 'T',\n",
       " 'AXR',\n",
       " 'EY',\n",
       " '+LAUGH+',\n",
       " 'F',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'S',\n",
       " 'IY',\n",
       " 'G',\n",
       " 'OW',\n",
       " 'L',\n",
       " 'D',\n",
       " 'AX',\n",
       " 'N',\n",
       " 'L',\n",
       " 'IH',\n",
       " 'V',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'R',\n",
       " 'UW',\n",
       " 'M',\n",
       " '+NOISE+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " '+HUMAN+',\n",
       " 'GARBAGE',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'Z',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'N',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'IY',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'F',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'K',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'P',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'R',\n",
       " 'SIL',\n",
       " '+LAUGH+',\n",
       " 'EY',\n",
       " 'SIL',\n",
       " '+LAUGH+',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'P',\n",
       " '+LAUGH+',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+HUMAN+',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " 'SIL',\n",
       " '+BREATH+',\n",
       " 'SIL',\n",
       " 'EY',\n",
       " 'W',\n",
       " 'UH',\n",
       " 'M',\n",
       " 'AX',\n",
       " 'N',\n",
       " 'W',\n",
       " 'EY',\n",
       " 'R',\n",
       " 'IX',\n",
       " 'NG',\n",
       " 'EY',\n",
       " 'B',\n",
       " 'L',\n",
       " 'UW',\n",
       " 'SH',\n",
       " 'ER',\n",
       " 'T',\n",
       " 'S',\n",
       " 'IH',\n",
       " 'T',\n",
       " 'S',\n",
       " ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'XL', 'HH', 'AY', 'F', 'IH', 'XM', 'P', 'ZH', 'AXR', 'EY', 'AO', '+LAUGH+', 'JH', 'DH', 'R', 'V', 'IY', 'CH', 'ER', 'TH', 'NG', 'D', 'SH', 'K', 'UH', 'S', 'SIL', 'AX', 'EH', 'T', 'B', 'AA', '+NOISE+', 'XN', 'UW', 'Z', 'M', '+HUMAN+', 'OW', 'Y', 'GARBAGE', 'AW', 'N', 'L', 'AH', 'IX', '+BREATH+', 'AE', 'OY', 'G']\n"
     ]
    }
   ],
   "source": [
    "myset = set(wordlist)\n",
    "myset = list(myset)\n",
    "print(myset)\n",
    "\n",
    "#there are 45 phonemes in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_phoneme_dictionary(listtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting dictionary overlap\n",
    "count = 0\n",
    "goodlist = []\n",
    "for words in listtest2:\n",
    "    if phonemedictionary[words] == dict1[words]:\n",
    "        goodlist.append(words)\n",
    "        count = count + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMultiple(mainstring, replacelist, replacevalue):\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in replacelist :\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainstring :\n",
    "            # Replace the string\n",
    "            mainstring = mainstring.replace(elem, replacevalue)\n",
    "    \n",
    "    return  mainstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dictionary provided by markus mueller\n",
    "file = \"/Users/sebastiaanscholten/Documents/speech2image-master/vgsexperiments/experiments/data/flickr8.dict.txt\"\n",
    "markusdictionary = defaultdict(list)\n",
    "with open(file, \"r\") as f:\n",
    "    for line in f:\n",
    "        #check if the word ends with - otherwise just take out first string and lower()\n",
    "        if line.split(\" \",-1)[0].lower()[-1] == \"-\":\n",
    "            word = line.split(\" \",-1)[0].lower()[:-1]\n",
    "        elif line.split(\" \",-1)[0].lower()[-1] == \")\":\n",
    "            word = re.sub(r'\\(([^)]+)\\)', '', line.split(\" \",-1)[0].lower())\n",
    "        else:\n",
    "            word = line.split(\" \",-1)[0].lower()\n",
    "        #parse out unneeded information\n",
    "        phonemes = replaceMultiple(line, [\"\\n\",\"{\",\"}\",\"WB\"], \"\")\n",
    "        phonemes = re.sub(' +', ' ', phonemes)[:-1]\n",
    "        phonemes = phonemes.split(\" \")[1:]\n",
    "        \n",
    "        markusdictionary[word].append(phonemes)\n",
    "#to remove list in list duplicates\n",
    "for key, value in markusdictionary.items():\n",
    "    b_set = set(tuple(x) for x in value)\n",
    "    b = [ list(x) for x in b_set ]\n",
    "    markusdictionary[key] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['P', 'IY', 'P', 'XL']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markusdictionary[\"people\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
